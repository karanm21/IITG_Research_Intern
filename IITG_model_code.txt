# -*- coding: utf-8 -*-
"""IITG_Summers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_KhaTN7DD9mu1YJ9tPxumRAkb_dGBk3O
"""

# pip install wfdb

import wfdb

# Commented out IPython magic to ensure Python compatibility.
from IPython.display import display
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import os
import shutil
import posixpath
import pandas as pd
#%%
# wfdb.dl_database('mitdb', os.path.join(os.getcwd(), 'mitdb'))
#%%
from wfdb import processing
import pywt
import sys
from scipy import signal
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [30, 15]
from sklearn import svm
# from sklearn_rvm import EMRVC
# from skrvm import RVC
from scipy.signal import find_peaks
from scipy.signal import decimate
from sklearn.decomposition import PCA
import neurokit2 as nk
import biosppy
from statistics import mean
# wfdb.show_ann_labels()
#%%

path='/home/meenali/Meenali/Edited_ANN_July_2021/mit-bih-arrhythmia-database-1.0.0/'
records = ['100','101','102','103','104','100','106','107',
            '108','109','111','112','113','114','115','116','117',
            '118','119','121','122','123','124','200','201','202','203',
            '205','207','208','209','210','212','213','215','217','219',
            '220','221','222','223','228','230','231','232','233','234'] 
   
# records = ['mitdb/100','mitdb/101','mitdb/102', 'mitdb/103', 'mitdb/100', 'mitdb/106', 'mitdb/111', 'mitdb/112', 
#                  'mitdb/113','mitdb/115', 'mitdb/116', 'mitdb/118', 'mitdb/121', 'mitdb/122',  'mitdb/123', 'mitdb/124',
#                  'mitdb/209','mitdb/210','mitdb/230','mitdb/231', 'mitdb/232', 'mitdb/234',
#                  'mitdb/221','mitdb/222', 'mitdb/223', 'mitdb/228']
# pip install neurokit2

# pip install biosppy



def startFuncMIT_V(signalString):
    
    signals, fields = wfdb.rdsamp(signalString)
    signals = signals[:,0]
    signals = np.array(signals, dtype=[('O', float)]).astype(float)
    signals = (signals - np.mean(signals))/(np.std(signals))
    ann = wfdb.rdann(signalString, 'atr')
    
    signals, ann = wfdb.processing.resample_singlechan(signals, ann, 360, 250)
    samples = ann.sample
    symbols = ann.symbol
    
    points = []
    pointSym = []
    
    for i in range(len(samples)):
        if(symbols[i] == 'V' or symbols[i] == 'F' or symbols[i] == 'E' or symbols[i] == '!'):
            points.append(samples[i])
            pointSym.append(symbols[i])
    
    return points, pointSym, signals, samples

def beats(points, rpeaks, signals,offset, listV, listN):
  for i in range(len(points)):
    index=points[i]
    midindex= index-offset
    lastindex= midindex-offset

    if(i==0 and midindex<0):
        midindex=0
        lastindex=0

    elif(i==0 and midindex > 0 and lastindex < 0):
      lastindex=0;

    elif(i>0 and midindex < points[i-1]):
      midindex= points[i-1]+1
      lastindex= points[i-1]

    elif(i>0 and lastindex < points[i-1]):
      lastindex= points[i-1]+1

    a=lastindex+1
    b=midindex
    c=index

    R_points= np.array([])
    RR= np.array([])

    for j in range(1,len(rpeaks)):
      ind= rpeaks[j];
      if(ind>0 and ind>=b and ind<=c):
        RR= np.append(RR, int((rpeaks[j]+rpeaks[j-1])/2))
        R_points= np.append(R_points, ind)
      elif(ind>c):
        break

    for i in range(len(RR)):
      list1= np.array([])
      
      dis1= int(R_points[i]-RR[i])
      if((i+1)<len(RR)):
        dis2= int(RR[i+1]- R_points[i])
      else:
        dis2=0     

      k=int(R_points[i]) 
      
      if(dis1>100 and dis2>100):
        list1= np.concatenate((list1, signals[k-100:k+100]), axis=None)
      
      elif(dis1<100 and dis2 >100):
        list1= np.concatenate((list1, np.zeros(100-dis1, dtype='float')), axis=None)
        list1= np.concatenate((list1, signals[k-dis1:k+100]), axis=None)

      elif(dis1<100 and dis2<100 and dis2!=0):
        list1= np.concatenate((list1, np.zeros(100-dis1, dtype='float')), axis=None)
        list1= np.concatenate((list1, signals[k-dis1:k+dis2]), axis=None)
        list1= np.concatenate((list1, np.zeros(100-dis2, dtype='float')), axis=None)

      elif(dis1>100 and dis2<100 and dis2!=0):
        list1= np.concatenate((list1, signals[k-100: k+dis2]), axis=None)
        list1= np.concatenate((list1, np.zeros(100-dis2, dtype='float')), axis=None)

      elif(dis1>100 and dis2==0):
        list1= np.concatenate((list1, signals[k-100:k]), axis=None)
        list1= np.concatenate((list1, np.zeros(100, dtype='float')), axis=None)

      elif(dis1<100 and dis2==0):
        list1= np.concatenate((list1, np.zeros(100-dis1, dtype='float')), axis=None)
        list1= np.concatenate((list1, signals[k-dis1:k]), axis=None)
        list1= np.concatenate((list1, np.zeros(100, dtype='float')), axis=None)

      listV.append(list1)

    R_points= np.array([])
    RR= np.array([])

    for k in range(1,len(rpeaks)):
        ind= rpeaks[k];
        if(ind>0 and ind>=a and ind<=b):
          RR= np.append(RR, int((rpeaks[k]+rpeaks[k-1])/2))
          R_points= np.append(R_points, ind)
        elif(ind>b):
          break;

    for l in range(len(RR)):
      list1= np.array([])
      
      dis1= int(R_points[l]-RR[l])
      if((l+1)<len(RR)):
        dis2= int(RR[l+1]- R_points[l])
      else:
        dis2=0     

      k=int(R_points[l]) 
      
      if(dis1>100 and dis2>100):
        list1= np.concatenate((list1, signals[k-100:k+100]), axis=None)
      
      elif(dis1<100 and dis2 >100):
        list1= np.concatenate((list1, np.zeros(100-dis1, dtype='float')), axis=None)
        list1= np.concatenate((list1, signals[k-dis1:k+100]), axis=None)

      elif(dis1<100 and dis2<100 and dis2!=0):
        list1= np.concatenate((list1, np.zeros(100-dis1, dtype='float')), axis=None)
        list1= np.concatenate((list1, signals[k-dis1:k+dis2]), axis=None)
        list1= np.concatenate((list1, np.zeros(100-dis2, dtype='float')), axis=None)

      elif(dis1>100 and dis2<100 and dis2!=0):
        list1= np.concatenate((list1, signals[k-100: k+dis2]), axis=None)
        list1= np.concatenate((list1, np.zeros(100-dis2, dtype='float')), axis=None)

      elif(dis1>100 and dis2==0):
        list1= np.concatenate((list1, signals[k-100:k]), axis=None)
        list1= np.concatenate((list1, np.zeros(100, dtype='float')), axis=None)

      elif(dis1<100 and dis2==0):
        list1= np.concatenate((list1, np.zeros(100-dis1, dtype='float')), axis=None)
        list1= np.concatenate((list1, signals[k-dis1:k]), axis=None)
        list1= np.concatenate((list1, np.zeros(100, dtype='float')), axis=None)

      listN.append(list1)

def get_Beats_V(record, listV, listN):

  points, pointSym, signals, samples = startFuncMIT_V(path+record)
  cleaned = nk.ecg_clean(signals, sampling_rate = 250)
  rdet, = biosppy.signals.ecg.hamilton_segmenter(signal = cleaned, sampling_rate = 250)   
  rdet, = biosppy.signals.ecg.correct_rpeaks(signal = cleaned, rpeaks = rdet, sampling_rate = 250, tol = 0.05)

  rdet = np.delete(rdet, -1) 
  rdet = np.delete(rdet, 0) 

  R = rdet
  rpeaks= []
  nanindices=[]

  for peak in range(len(R)):
    if(np.isnan(R[peak])):
      nanindices.append(peak)
    else:
      rpeaks.append(R[peak])

  param=15
  fs= 250
  offset= param*fs*60

  beats(points, rpeaks, signals, offset, listV, listN)

def startFuncMIT_S(signalString):
    
    signals, fields = wfdb.rdsamp(signalString)
    signals = signals[:,0]
    signals = np.array(signals, dtype=[('O', float)]).astype(float)
    signals = (signals - np.mean(signals))/(np.std(signals))
    ann = wfdb.rdann(signalString, 'atr')
    
    signals, ann = wfdb.processing.resample_singlechan(signals, ann, 360, 250)
    samples = ann.sample
    symbols = ann.symbol
    
    points = []
    pointSym = []
    
    for i in range(len(samples)):
        if(symbols[i] == 'a' or symbols[i] == 'A' or symbols[i] == 'e'):
            points.append(samples[i])
            pointSym.append(symbols[i])
    
    return points, pointSym, signals, samples

def get_Beats_S(record, listV, listN):

  points, pointSym, signals, samples = startFuncMIT_S(path+record)
  cleaned = nk.ecg_clean(signals, sampling_rate = 250)
  rdet, = biosppy.signals.ecg.hamilton_segmenter(signal = cleaned, sampling_rate = 250)   
  rdet, = biosppy.signals.ecg.correct_rpeaks(signal = cleaned, rpeaks = rdet, sampling_rate = 250, tol = 0.05)

  rdet = np.delete(rdet, -1) 
  rdet = np.delete(rdet, 0) 

  R = rdet
  rpeaks= []
  nanindices=[]

  for peak in range(len(R)):
    if(np.isnan(R[peak])):
      nanindices.append(peak)
    else:
      rpeaks.append(R[peak])

  param=15
  fs= 250
  offset= param*fs*60

  beats(points, rpeaks, signals, offset, listV, listN)

listV= []
listN= []
listS= []

for record in records:
  # rec=path+record
  get_Beats_V(record, listV, listN)
  get_Beats_S(record, listS, listN)

len(listV)

len(listS)

len(listN)
dfV= pd.DataFrame(listV)
dfV['Type']='V'

dfV.head()

dfN= pd.DataFrame(listN)
dfN['Type']= 'N'
dfN.head()

dfS= pd.DataFrame(listS)
dfS['Type']= 'S'
dfS.head()
pdList = [dfV,dfS,dfN]  # List of your dataframes
df = pd.concat(pdList, ignore_index=False)

df.isnull().sum()
df.dropna(inplace=True)
#%%
df = df.sample(frac=1).reset_index(drop=True)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow import keras

# Assuming your DataFrame is called 'df'

# Splitting the data into features (X) and labels (y)
X = df.iloc[:, :-1].values  # Features (all columns except the last one)
y = df.iloc[:, -1].values   # Labels (last column)

# Perform one-hot encoding of labels
y_encoded = pd.get_dummies(y)

# Splitting into training and test data
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# # Normalize the features
# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)

# Creating the DNN model
model = keras.Sequential()
model.add(keras.layers.Dense(300, activation='relu', input_shape=(X_train.shape[1],)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(50, activation='relu'))
model.add(keras.layers.Dense(15, activation='relu'))
model.add(keras.layers.Dense(3, activation='softmax'))

# Compiling the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Training the model
model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test,Â y_test))